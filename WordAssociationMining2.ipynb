{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "hm5mOWS5Y-LY",
        "outputId": "e7a6f011-5c48-44ec-cde8-4acfd5c6e4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.10.0\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5 scipy-1.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2e6a3502b6484e7788a2dcbbad91259e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U scipy==1.10.0 numpy==1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U git+https://github.com/stephenhky/PyShortTextCategorization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPn--U6kZDjw",
        "outputId": "5968dfc1-3f2e-4d4a-d202-862b8249c190"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/stephenhky/PyShortTextCategorization\n",
            "  Cloning https://github.com/stephenhky/PyShortTextCategorization to /tmp/pip-req-build-das3mn_0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/stephenhky/PyShortTextCategorization /tmp/pip-req-build-das3mn_0\n",
            "  Resolved https://github.com/stephenhky/PyShortTextCategorization to commit e183bac2a362051087e3cb9160ccd8f2ee67f315\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (1.10.0)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (1.2.2)\n",
            "Requirement already satisfied: tensorflow>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (2.15.0)\n",
            "Requirement already satisfied: keras>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (2.15.0)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (4.3.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (1.5.3)\n",
            "Requirement already satisfied: snowballstemmer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (2.2.0)\n",
            "Requirement already satisfied: transformers>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (4.38.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (2.1.0+cu121)\n",
            "Collecting python-Levenshtein>=0.21.0 (from shorttext==1.6.1)\n",
            "  Downloading python_Levenshtein-0.25.0-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.10/dist-packages (from shorttext==1.6.1) (0.58.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->shorttext==1.6.1) (6.4.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57.0->shorttext==1.6.1) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->shorttext==1.6.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->shorttext==1.6.1) (2023.4)\n",
            "Collecting Levenshtein==0.25.0 (from python-Levenshtein>=0.21.0->shorttext==1.6.1)\n",
            "  Downloading Levenshtein-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.25.0->python-Levenshtein>=0.21.0->shorttext==1.6.1)\n",
            "  Downloading rapidfuzz-3.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->shorttext==1.6.1) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (24.3.6)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->shorttext==1.6.1) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->shorttext==1.6.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->shorttext==1.6.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->shorttext==1.6.1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->shorttext==1.6.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->shorttext==1.6.1) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.32.0->shorttext==1.6.1) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.32.0->shorttext==1.6.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.32.0->shorttext==1.6.1) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.32.0->shorttext==1.6.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.32.0->shorttext==1.6.1) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.32.0->shorttext==1.6.1) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.32.0->shorttext==1.6.1) (4.66.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.13.0->shorttext==1.6.1) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.32.0->shorttext==1.6.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.32.0->shorttext==1.6.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.32.0->shorttext==1.6.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.32.0->shorttext==1.6.1) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->shorttext==1.6.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->shorttext==1.6.1) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1) (3.2.2)\n",
            "Building wheels for collected packages: shorttext\n",
            "  Building wheel for shorttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shorttext: filename=shorttext-1.6.1-cp310-cp310-linux_x86_64.whl size=724849 sha256=2e718c1de6b815bee58c3bd5523fbae57c748f5c25c4cedf40c8f7f73f7e0e97\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qsh7a43a/wheels/84/07/3e/bb56d79459e5c3c9111c649d174481179ef9258e9559bdd72c\n",
            "Successfully built shorttext\n",
            "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein, shorttext\n",
            "Successfully installed Levenshtein-0.25.0 python-Levenshtein-0.25.0 rapidfuzz-3.6.2 shorttext-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO0xLFHuZHrz",
        "outputId": "aa073108-da77-488b-abed-9c56c872ada9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wp3wpfWZwKn",
        "outputId": "6790cb6e-86ef-4354-9230-fae1d68fb1ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/DSBA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcsYVtqPZ2zZ",
        "outputId": "02b70d44-ea97-4752-fd48-a3635bb44847"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/DSBA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "corpus_root = 'MovieReviews' # Folder Name\n",
        "filelists = PlaintextCorpusReader(corpus_root, '.*',encoding='latin-1')  # wildcard is read all files in the folder\n",
        "filelists.fileids()  # Get the filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEQS4_m-Z5Wo",
        "outputId": "64066b07-2021-4687-c1f7-ffc04e8f7f45"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MovieReviews/16748.txt',\n",
              " 'MovieReviews/17108.txt',\n",
              " 'MovieReviews/17109.txt',\n",
              " 'MovieReviews/17110.txt',\n",
              " 'MovieReviews/17111.txt',\n",
              " 'MovieReviews/17116.txt',\n",
              " 'MovieReviews/17117.txt',\n",
              " 'MovieReviews/17118.txt',\n",
              " 'MovieReviews/17119.txt',\n",
              " 'MovieReviews/17139.txt',\n",
              " 'MovieReviews/17144.txt',\n",
              " 'MovieReviews/17145.txt',\n",
              " 'MovieReviews/17146.txt',\n",
              " 'MovieReviews/17147.txt',\n",
              " 'MovieReviews/17150.txt',\n",
              " 'MovieReviews/17185.txt',\n",
              " 'MovieReviews/17192.txt',\n",
              " 'MovieReviews/17219.txt',\n",
              " 'MovieReviews/17239.txt',\n",
              " 'MovieReviews/17243.txt',\n",
              " 'MovieReviews/17254.txt',\n",
              " 'MovieReviews/17255.txt',\n",
              " 'MovieReviews/17280.txt',\n",
              " 'MovieReviews/17300.txt',\n",
              " 'MovieReviews/17303.txt',\n",
              " 'MovieReviews/17341.txt',\n",
              " 'MovieReviews/17384.txt',\n",
              " 'MovieReviews/17391.txt',\n",
              " 'MovieReviews/17398.txt',\n",
              " 'MovieReviews/17399.txt',\n",
              " 'MovieReviews/17430.txt',\n",
              " 'MovieReviews/17431.txt',\n",
              " 'MovieReviews/17447.txt',\n",
              " 'MovieReviews/17457.txt',\n",
              " 'MovieReviews/17460.txt',\n",
              " 'MovieReviews/17501.txt',\n",
              " 'MovieReviews/17518.txt',\n",
              " 'MovieReviews/17532.txt',\n",
              " 'MovieReviews/17534.txt',\n",
              " 'MovieReviews/17578.txt',\n",
              " 'MovieReviews/17609.txt',\n",
              " 'MovieReviews/17610.txt',\n",
              " 'MovieReviews/17655.txt',\n",
              " 'MovieReviews/17662.txt',\n",
              " 'MovieReviews/17663.txt',\n",
              " 'MovieReviews/17695.txt',\n",
              " 'MovieReviews/17711.txt',\n",
              " 'MovieReviews/17713.txt',\n",
              " 'MovieReviews/17753.txt',\n",
              " 'MovieReviews/17757.txt',\n",
              " 'MovieReviews/17758.txt',\n",
              " 'MovieReviews/17761.txt',\n",
              " 'MovieReviews/17803.txt',\n",
              " 'MovieReviews/17811.txt',\n",
              " 'MovieReviews/17874.txt',\n",
              " 'MovieReviews/17879.txt',\n",
              " 'MovieReviews/17886.txt',\n",
              " 'MovieReviews/17896.txt',\n",
              " 'MovieReviews/17898.txt',\n",
              " 'MovieReviews/17902.txt',\n",
              " 'MovieReviews/17912.txt',\n",
              " 'MovieReviews/17933.txt',\n",
              " 'MovieReviews/17934.txt',\n",
              " 'MovieReviews/17945.txt',\n",
              " 'MovieReviews/17963.txt',\n",
              " 'MovieReviews/17971.txt',\n",
              " 'MovieReviews/17992.txt',\n",
              " 'MovieReviews/18004.txt',\n",
              " 'MovieReviews/18016.txt',\n",
              " 'MovieReviews/18032.txt',\n",
              " 'MovieReviews/18067.txt',\n",
              " 'MovieReviews/18068.txt',\n",
              " 'MovieReviews/18080.txt',\n",
              " 'MovieReviews/18087.txt',\n",
              " 'MovieReviews/18088.txt',\n",
              " 'MovieReviews/18136.txt',\n",
              " 'MovieReviews/18141.txt',\n",
              " 'MovieReviews/18156.txt',\n",
              " 'MovieReviews/18161.txt',\n",
              " 'MovieReviews/18181.txt',\n",
              " 'MovieReviews/18227.txt',\n",
              " 'MovieReviews/18263.txt',\n",
              " 'MovieReviews/18272.txt',\n",
              " 'MovieReviews/18273.txt',\n",
              " 'MovieReviews/18274.txt',\n",
              " 'MovieReviews/18282.txt',\n",
              " 'MovieReviews/18283.txt',\n",
              " 'MovieReviews/18307.txt',\n",
              " 'MovieReviews/18368.txt',\n",
              " 'MovieReviews/18375.txt',\n",
              " 'MovieReviews/18376.txt',\n",
              " 'MovieReviews/18396.txt',\n",
              " 'MovieReviews/18406.txt',\n",
              " 'MovieReviews/18413.txt',\n",
              " 'MovieReviews/18414.txt',\n",
              " 'MovieReviews/18447.txt',\n",
              " 'MovieReviews/18473.txt',\n",
              " 'MovieReviews/18480.txt',\n",
              " 'MovieReviews/18485.txt',\n",
              " 'MovieReviews/18498.txt',\n",
              " 'MovieReviews/1858.txt',\n",
              " 'MovieReviews/1859.txt',\n",
              " 'MovieReviews/1860.txt',\n",
              " 'MovieReviews/1864.txt',\n",
              " 'MovieReviews/1865.txt',\n",
              " 'MovieReviews/1866.txt',\n",
              " 'MovieReviews/1867.txt',\n",
              " 'MovieReviews/1889.txt',\n",
              " 'MovieReviews/1891.txt',\n",
              " 'MovieReviews/1908.txt',\n",
              " 'MovieReviews/1910.txt',\n",
              " 'MovieReviews/1911.txt',\n",
              " 'MovieReviews/1912.txt',\n",
              " 'MovieReviews/1916.txt',\n",
              " 'MovieReviews/1917.txt',\n",
              " 'MovieReviews/1921.txt',\n",
              " 'MovieReviews/1925.txt',\n",
              " 'MovieReviews/1928.txt',\n",
              " 'MovieReviews/1929.txt',\n",
              " 'MovieReviews/1930.txt',\n",
              " 'MovieReviews/1932.txt',\n",
              " 'MovieReviews/1934.txt',\n",
              " 'MovieReviews/1937.txt',\n",
              " 'MovieReviews/1943.txt',\n",
              " 'MovieReviews/1944.txt',\n",
              " 'MovieReviews/1945.txt',\n",
              " 'MovieReviews/1961.txt',\n",
              " 'MovieReviews/1967.txt',\n",
              " 'MovieReviews/1968.txt',\n",
              " 'MovieReviews/1974.txt',\n",
              " 'MovieReviews/1975.txt',\n",
              " 'MovieReviews/1976.txt',\n",
              " 'MovieReviews/1979.txt',\n",
              " 'MovieReviews/1981.txt',\n",
              " 'MovieReviews/1984.txt',\n",
              " 'MovieReviews/1985.txt',\n",
              " 'MovieReviews/1990.txt',\n",
              " 'MovieReviews/1991.txt',\n",
              " 'MovieReviews/1994.txt',\n",
              " 'MovieReviews/1998.txt',\n",
              " 'MovieReviews/2005.txt',\n",
              " 'MovieReviews/2006.txt',\n",
              " 'MovieReviews/2007.txt',\n",
              " 'MovieReviews/2008.txt',\n",
              " 'MovieReviews/2009.txt',\n",
              " 'MovieReviews/2025.txt',\n",
              " 'MovieReviews/2026.txt',\n",
              " 'MovieReviews/2030.txt',\n",
              " 'MovieReviews/2031.txt',\n",
              " 'MovieReviews/2033.txt',\n",
              " 'MovieReviews/2035.txt',\n",
              " 'MovieReviews/2036.txt',\n",
              " 'MovieReviews/2043.txt',\n",
              " 'MovieReviews/2045.txt',\n",
              " 'MovieReviews/2046.txt',\n",
              " 'MovieReviews/2051.txt',\n",
              " 'MovieReviews/2055.txt',\n",
              " 'MovieReviews/2058.txt',\n",
              " 'MovieReviews/2059.txt',\n",
              " 'MovieReviews/2062.txt',\n",
              " 'MovieReviews/2067.txt',\n",
              " 'MovieReviews/2076.txt',\n",
              " 'MovieReviews/2079.txt',\n",
              " 'MovieReviews/2080.txt',\n",
              " 'MovieReviews/2081.txt',\n",
              " 'MovieReviews/2085.txt',\n",
              " 'MovieReviews/2086.txt',\n",
              " 'MovieReviews/2087.txt',\n",
              " 'MovieReviews/2089.txt',\n",
              " 'MovieReviews/2090.txt',\n",
              " 'MovieReviews/2091.txt',\n",
              " 'MovieReviews/2094.txt',\n",
              " 'MovieReviews/2095.txt',\n",
              " 'MovieReviews/2098.txt',\n",
              " 'MovieReviews/2099.txt',\n",
              " 'MovieReviews/2101.txt',\n",
              " 'MovieReviews/2108.txt',\n",
              " 'MovieReviews/2110.txt',\n",
              " 'MovieReviews/2113.txt',\n",
              " 'MovieReviews/2115.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a list of movie reviews\n",
        "reviews = []\n",
        "for fileid in filelists.fileids():\n",
        "    reviews.append(filelists.raw(fileid))"
      ],
      "metadata": {
        "id": "zTSvOaRZaOSG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "Spo_b4HzaWEq",
        "outputId": "e1e118fb-d405-4447-eac4-0a1f22fbc3c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'THE BED YOU SLEEP IN (director: Jon Jost; cast: Ray Weiss (Tom Blair), Doug (Marshall Gaddis), Beth (Kate Sannella), Scott (Brad Shelton), Mrs. Weiss (Ellen McLaughlin), 1993)\\nTruly independent filmmaker, Jost, has completed his so-called trilogy about rural America with this film and has since moved on to self- imposed exile by going to Europe. This extra-ordinary film offers a long hard look at its subject matter, as the camera is held steadfast, not moving for insatiably long periods of time, picking up all the appropriate nuances it needs to with deliberate dispassion, as it looks at an Oregon lumber mill, whose owner (Tom Blair) is faced with unsettling economic news about the business he has built-up and worked at for his 50- odd years of life. It focuses on this man and tries to find out who he is, using him as a metaphoric symbol for America, perhaps, emulating Emerson\\'s views, as his writings are flashed on screen, exhibiting some sayings from his essays on nature and America.\\nBy seeing who this man is, we get to see how he adjusts to his carefully scripted life, the fly-fishing he loves to do for the sport of it, his easy and almost gentile manners, and his very definite American persona, as he is forced out of economic necessity to deal with the Japanese businessmen he inherently despises, and we get a picture of a rather complicated individual, who has difficulty in communicating with himself and others, so the closer we get to him, the more we sense that there are a lot of things about him that remain unanswered. The shocker about his life that is about to unfold, comes after he meets a foreign stranger on the street, raving about the day of atonement coming soon and how God knows all, that he should pray with him, but is told by him that he has no time for that, as he feels uncomfortable being around this religious zealot, so he fumbles around with his wad of bills and thrusts a few dollars in the preacher\\'s pockets, that are not kindly received by the preacher, as he quickly departs from the preacher\\'s shouts that he doesn\\'t want his money.\\nOur perceptions of him, as a Rock of Gibraltor type, is squelched for good, as we see him come unglued in his very comfortable home, as he interacts with his wife (Ellen McLaughlin), his second wife, as she confronts him with a letter from her college-aged daughter, Tracy, who is his daughter via his first marriage and therefore not Ellen\\'s real daughter. Ellen insists on reading a letter addressed to her from Tracy, out loud, accusing him of placing his hands on her private places, as he responds to his wife\\'s question, all she wants to know, is it true? And all he can respond, is that he wonders why Tracy is doing this to him, saying that she is probably mixed up. What results is apocalyptic, as the film becomes disturbingly mysterious and evasive, never settling for sure who is telling the truth, but destroying the family as it is. This scene could also be deemed as an attack on America\\'s soul, exposing it to questions about truth and character, principles that are put under the microscope, as the story builds to its very tragic outcome.\\nThis is one of Jost\\'s deepest and most penetrating films, it could even be argued that he has made a classical film, as it forcefully and subtly tells an American story, replete with unanswered questions about family life that are haunting, that give the film a certain power that makes you think for a long time afterwards what is it about this country that is so raw and violent in nature, that becomes a part of the people\\'s own nature.\\nOne of the scenes that I found most memorable, was when the camera panned the diner where Tom was dining with some co-workers and all we could hear, at first, was the muffled conversations of the patrons, as the camera meticulously panned the diner, until the atmosphere of the place was fully absorbed and we returned to Tom and his conversation, which became clearer, as this scene played out a daily experience most Americans have had but has rarely been captured so exactly on film. This time consuming shot, is not attempted by commercial filmmakers who live in fear of losing their audience in a long non-action shot.This is one of Jost\\'s strong points, his willingness to explore territory others fear to go.\\nJost\\'s film can be justifiably criticized for a few lapses in the story line it didn\\'t clarify more precisely, but more importantly, it should be praised for the poetry it brings to its story when telling about a malaise in the American culture that is difficult to come to grips with, as the American landscape is perceived as so beautiful a sight to behold and the country as so wealthy a place when compared with the rest of the world, as it asks... But, what does this mean if Americans are not a happy people?\\nREVIEWED ON 3/20/99\\nDennis Schwartz: \"Movie Reviews\"\\n=A9 ALL RIGHTS RESERVED DENNIS SCHWARTZ\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the shorttext library for text preprocessing\n",
        "#standard_text_preprocessor_1 under shorttext.utils provides a standar way of text preprocessing, including the following steps:\n",
        "\n",
        "   #1. removing special characters,\n",
        "   #2. removing numerals,\n",
        "   #3. converting all alphabets to lower cases,\n",
        "   #4. removing stop words, and\n",
        "   #5. stemming the words (using Porter stemmer).\n",
        "\n",
        "from shorttext.utils import standard_text_preprocessor_1, DocumentTermMatrix\n",
        "preprocessor = standard_text_preprocessor_1()\n",
        "corpus = [preprocessor(article).split(' ') for article in reviews]"
      ],
      "metadata": {
        "id": "pkW9CjK2aZHp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9GExRKMaeZJ",
        "outputId": "184d7e87-93e1-4d97-ac0d-30558715c8b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bed',\n",
              " 'sleep',\n",
              " 'director',\n",
              " 'jon',\n",
              " 'jost',\n",
              " 'cast',\n",
              " 'rai',\n",
              " 'weiss',\n",
              " 'tom',\n",
              " 'blair',\n",
              " 'doug',\n",
              " 'marshal',\n",
              " 'gaddi',\n",
              " 'beth',\n",
              " 'kate',\n",
              " 'sannella',\n",
              " 'scott',\n",
              " 'brad',\n",
              " 'shelton',\n",
              " 'mr',\n",
              " 'weiss',\n",
              " 'mclaughlin',\n",
              " '\\ntruli',\n",
              " 'independ',\n",
              " 'filmmak',\n",
              " 'jost',\n",
              " 'complet',\n",
              " 'socal',\n",
              " 'trilogi',\n",
              " 'rural',\n",
              " 'america',\n",
              " 'film',\n",
              " 'sinc',\n",
              " 'move',\n",
              " 'self',\n",
              " 'impos',\n",
              " 'exil',\n",
              " 'go',\n",
              " 'europ',\n",
              " 'extraordinari',\n",
              " 'film',\n",
              " 'offer',\n",
              " 'long',\n",
              " 'hard',\n",
              " 'look',\n",
              " 'subject',\n",
              " 'matter',\n",
              " 'camera',\n",
              " 'held',\n",
              " 'steadfast',\n",
              " 'move',\n",
              " 'insati',\n",
              " 'long',\n",
              " 'period',\n",
              " 'time',\n",
              " 'pick',\n",
              " 'appropri',\n",
              " 'nuanc',\n",
              " 'need',\n",
              " 'deliber',\n",
              " 'dispass',\n",
              " 'look',\n",
              " 'oregon',\n",
              " 'lumber',\n",
              " 'mill',\n",
              " 'whose',\n",
              " 'owner',\n",
              " 'tom',\n",
              " 'blair',\n",
              " 'face',\n",
              " 'unsettl',\n",
              " 'econom',\n",
              " 'new',\n",
              " 'busi',\n",
              " 'builtup',\n",
              " 'work',\n",
              " '',\n",
              " 'odd',\n",
              " 'year',\n",
              " 'life',\n",
              " 'focus',\n",
              " 'tri',\n",
              " 'find',\n",
              " 'us',\n",
              " 'metaphor',\n",
              " 'symbol',\n",
              " 'america',\n",
              " 'perhap',\n",
              " 'emul',\n",
              " 'emerson',\n",
              " 'view',\n",
              " 'write',\n",
              " 'flash',\n",
              " 'screen',\n",
              " 'exhibit',\n",
              " 'sai',\n",
              " 'essai',\n",
              " 'natur',\n",
              " 'america\\nbi',\n",
              " 'see',\n",
              " 'get',\n",
              " 'see',\n",
              " 'adjust',\n",
              " 'carefulli',\n",
              " 'script',\n",
              " 'life',\n",
              " 'flyfish',\n",
              " 'love',\n",
              " 'sport',\n",
              " 'easi',\n",
              " 'almost',\n",
              " 'gentil',\n",
              " 'manner',\n",
              " 'definit',\n",
              " 'american',\n",
              " 'persona',\n",
              " 'forc',\n",
              " 'econom',\n",
              " 'necess',\n",
              " 'deal',\n",
              " 'japanes',\n",
              " 'businessmen',\n",
              " 'inher',\n",
              " 'despis',\n",
              " 'get',\n",
              " 'pictur',\n",
              " 'rather',\n",
              " 'complic',\n",
              " 'individu',\n",
              " 'difficulti',\n",
              " 'commun',\n",
              " 'other',\n",
              " 'closer',\n",
              " 'get',\n",
              " 'sens',\n",
              " 'lot',\n",
              " 'thing',\n",
              " 'remain',\n",
              " 'unansw',\n",
              " 'shocker',\n",
              " 'life',\n",
              " 'unfold',\n",
              " 'come',\n",
              " 'meet',\n",
              " 'foreign',\n",
              " 'stranger',\n",
              " 'street',\n",
              " 'rave',\n",
              " 'dai',\n",
              " 'aton',\n",
              " 'come',\n",
              " 'soon',\n",
              " 'god',\n",
              " 'know',\n",
              " 'prai',\n",
              " 'told',\n",
              " 'time',\n",
              " 'feel',\n",
              " 'uncomfort',\n",
              " 'around',\n",
              " 'religi',\n",
              " 'zealot',\n",
              " 'fumbl',\n",
              " 'around',\n",
              " 'wad',\n",
              " 'bill',\n",
              " 'thrust',\n",
              " 'dollar',\n",
              " 'preacher',\n",
              " 'pocket',\n",
              " 'kindli',\n",
              " 'receiv',\n",
              " 'preacher',\n",
              " 'quickli',\n",
              " 'depart',\n",
              " 'preacher',\n",
              " 'shout',\n",
              " 'doesnt',\n",
              " 'money\\nour',\n",
              " 'percept',\n",
              " 'rock',\n",
              " 'gibraltor',\n",
              " 'type',\n",
              " 'squelch',\n",
              " 'good',\n",
              " 'see',\n",
              " 'unglu',\n",
              " 'comfort',\n",
              " 'home',\n",
              " 'interact',\n",
              " 'wife',\n",
              " 'mclaughlin',\n",
              " 'second',\n",
              " 'wife',\n",
              " 'confront',\n",
              " 'letter',\n",
              " 'collegeag',\n",
              " 'daughter',\n",
              " 'traci',\n",
              " 'daughter',\n",
              " 'via',\n",
              " 'first',\n",
              " 'marriag',\n",
              " 'therefor',\n",
              " 'ellen',\n",
              " 'real',\n",
              " 'daughter',\n",
              " 'insist',\n",
              " 'read',\n",
              " 'letter',\n",
              " 'address',\n",
              " 'traci',\n",
              " 'loud',\n",
              " 'accus',\n",
              " 'place',\n",
              " 'hand',\n",
              " 'privat',\n",
              " 'place',\n",
              " 'respond',\n",
              " 'wife',\n",
              " 'question',\n",
              " 'want',\n",
              " 'know',\n",
              " 'true',\n",
              " 'respond',\n",
              " 'wonder',\n",
              " 'traci',\n",
              " 'sai',\n",
              " 'probabl',\n",
              " 'mix',\n",
              " 'result',\n",
              " 'apocalypt',\n",
              " 'film',\n",
              " 'becom',\n",
              " 'disturbingli',\n",
              " 'mysteri',\n",
              " 'evas',\n",
              " 'never',\n",
              " 'settl',\n",
              " 'sure',\n",
              " 'tell',\n",
              " 'truth',\n",
              " 'destroi',\n",
              " 'famili',\n",
              " 'scene',\n",
              " 'could',\n",
              " 'deem',\n",
              " 'attack',\n",
              " 'america',\n",
              " 'soul',\n",
              " 'expos',\n",
              " 'question',\n",
              " 'truth',\n",
              " 'charact',\n",
              " 'principl',\n",
              " 'put',\n",
              " 'microscop',\n",
              " 'stori',\n",
              " 'build',\n",
              " 'tragic',\n",
              " 'outcome\\nthi',\n",
              " 'on',\n",
              " 'jost',\n",
              " 'deepest',\n",
              " 'penetr',\n",
              " 'film',\n",
              " 'could',\n",
              " 'even',\n",
              " 'argu',\n",
              " 'made',\n",
              " 'classic',\n",
              " 'film',\n",
              " 'forcefulli',\n",
              " 'subtli',\n",
              " 'tell',\n",
              " 'american',\n",
              " 'stori',\n",
              " 'replet',\n",
              " 'unansw',\n",
              " 'question',\n",
              " 'famili',\n",
              " 'life',\n",
              " 'haunt',\n",
              " 'give',\n",
              " 'film',\n",
              " 'certain',\n",
              " 'power',\n",
              " 'make',\n",
              " 'think',\n",
              " 'long',\n",
              " 'time',\n",
              " 'afterward',\n",
              " 'countri',\n",
              " 'raw',\n",
              " 'violent',\n",
              " 'natur',\n",
              " 'becom',\n",
              " 'part',\n",
              " 'peopl',\n",
              " 'nature\\non',\n",
              " 'scene',\n",
              " 'found',\n",
              " 'memor',\n",
              " 'camera',\n",
              " 'pan',\n",
              " 'diner',\n",
              " 'tom',\n",
              " 'dine',\n",
              " 'cowork',\n",
              " 'could',\n",
              " 'hear',\n",
              " 'first',\n",
              " 'muffl',\n",
              " 'convers',\n",
              " 'patron',\n",
              " 'camera',\n",
              " 'meticul',\n",
              " 'pan',\n",
              " 'diner',\n",
              " 'atmospher',\n",
              " 'place',\n",
              " 'fulli',\n",
              " 'absorb',\n",
              " 'return',\n",
              " 'tom',\n",
              " 'convers',\n",
              " 'becam',\n",
              " 'clearer',\n",
              " 'scene',\n",
              " 'plai',\n",
              " 'daili',\n",
              " 'experi',\n",
              " 'american',\n",
              " 'rare',\n",
              " 'captur',\n",
              " 'exactli',\n",
              " 'film',\n",
              " 'time',\n",
              " 'consum',\n",
              " 'shot',\n",
              " 'attempt',\n",
              " 'commerci',\n",
              " 'filmmak',\n",
              " 'live',\n",
              " 'fear',\n",
              " 'lose',\n",
              " 'audienc',\n",
              " 'long',\n",
              " 'nonact',\n",
              " 'shotthi',\n",
              " 'on',\n",
              " 'jost',\n",
              " 'strong',\n",
              " 'point',\n",
              " 'willing',\n",
              " 'explor',\n",
              " 'territori',\n",
              " 'other',\n",
              " 'fear',\n",
              " 'go\\njost',\n",
              " 'film',\n",
              " 'justifi',\n",
              " 'critic',\n",
              " 'laps',\n",
              " 'stori',\n",
              " 'line',\n",
              " 'didnt',\n",
              " 'clarifi',\n",
              " 'precis',\n",
              " 'importantli',\n",
              " 'prais',\n",
              " 'poetri',\n",
              " 'bring',\n",
              " 'stori',\n",
              " 'tell',\n",
              " 'malais',\n",
              " 'american',\n",
              " 'cultur',\n",
              " 'difficult',\n",
              " 'grip',\n",
              " 'american',\n",
              " 'landscap',\n",
              " 'perceiv',\n",
              " 'beauti',\n",
              " 'sight',\n",
              " 'behold',\n",
              " 'countri',\n",
              " 'wealthi',\n",
              " 'place',\n",
              " 'compar',\n",
              " 'rest',\n",
              " 'world',\n",
              " 'ask',\n",
              " 'mean',\n",
              " 'american',\n",
              " 'happi',\n",
              " 'people\\nreview',\n",
              " '\\ndenni',\n",
              " 'schwartz',\n",
              " 'movi',\n",
              " 'reviews\\na',\n",
              " 'right',\n",
              " 'reserv',\n",
              " 'denni',\n",
              " 'schwartz\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the corpus of news to a document term matrix (dtm), where each row represents a document, each column represents the number of occurence for a word\n",
        "dtm = DocumentTermMatrix(corpus, docids = filelists.fileids())"
      ],
      "metadata": {
        "id": "tDmz14afajJR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check number of occurence of the word \"director\" in each document\n",
        "dtm.get_token_occurences('director')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL3Ymd7jam5l",
        "outputId": "514bf7fa-e4df-44a4-a64f-e032c405df0e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MovieReviews/16748.txt': 3.0,\n",
              " 'MovieReviews/17108.txt': 3.0,\n",
              " 'MovieReviews/17109.txt': 1.0,\n",
              " 'MovieReviews/17110.txt': 2.0,\n",
              " 'MovieReviews/17111.txt': 1.0,\n",
              " 'MovieReviews/17116.txt': 1.0,\n",
              " 'MovieReviews/17117.txt': 5.0,\n",
              " 'MovieReviews/17118.txt': 1.0,\n",
              " 'MovieReviews/17119.txt': 5.0,\n",
              " 'MovieReviews/17139.txt': 1.0,\n",
              " 'MovieReviews/17144.txt': 1.0,\n",
              " 'MovieReviews/17145.txt': 2.0,\n",
              " 'MovieReviews/17146.txt': 2.0,\n",
              " 'MovieReviews/17147.txt': 1.0,\n",
              " 'MovieReviews/17150.txt': 1.0,\n",
              " 'MovieReviews/17185.txt': 3.0,\n",
              " 'MovieReviews/17192.txt': 4.0,\n",
              " 'MovieReviews/17219.txt': 1.0,\n",
              " 'MovieReviews/17239.txt': 2.0,\n",
              " 'MovieReviews/17243.txt': 1.0,\n",
              " 'MovieReviews/17254.txt': 1.0,\n",
              " 'MovieReviews/17255.txt': 1.0,\n",
              " 'MovieReviews/17280.txt': 2.0,\n",
              " 'MovieReviews/17300.txt': 2.0,\n",
              " 'MovieReviews/17303.txt': 2.0,\n",
              " 'MovieReviews/17341.txt': 1.0,\n",
              " 'MovieReviews/17384.txt': 2.0,\n",
              " 'MovieReviews/17398.txt': 1.0,\n",
              " 'MovieReviews/17399.txt': 3.0,\n",
              " 'MovieReviews/17430.txt': 2.0,\n",
              " 'MovieReviews/17431.txt': 2.0,\n",
              " 'MovieReviews/17447.txt': 1.0,\n",
              " 'MovieReviews/17457.txt': 1.0,\n",
              " 'MovieReviews/17460.txt': 2.0,\n",
              " 'MovieReviews/17501.txt': 1.0,\n",
              " 'MovieReviews/17518.txt': 1.0,\n",
              " 'MovieReviews/17534.txt': 1.0,\n",
              " 'MovieReviews/17578.txt': 1.0,\n",
              " 'MovieReviews/17609.txt': 1.0,\n",
              " 'MovieReviews/17610.txt': 1.0,\n",
              " 'MovieReviews/17655.txt': 1.0,\n",
              " 'MovieReviews/17663.txt': 1.0,\n",
              " 'MovieReviews/17695.txt': 2.0,\n",
              " 'MovieReviews/17711.txt': 1.0,\n",
              " 'MovieReviews/17713.txt': 2.0,\n",
              " 'MovieReviews/17753.txt': 1.0,\n",
              " 'MovieReviews/17757.txt': 2.0,\n",
              " 'MovieReviews/17758.txt': 2.0,\n",
              " 'MovieReviews/17761.txt': 1.0,\n",
              " 'MovieReviews/17803.txt': 1.0,\n",
              " 'MovieReviews/17811.txt': 1.0,\n",
              " 'MovieReviews/17874.txt': 2.0,\n",
              " 'MovieReviews/17879.txt': 1.0,\n",
              " 'MovieReviews/17886.txt': 3.0,\n",
              " 'MovieReviews/17896.txt': 1.0,\n",
              " 'MovieReviews/17898.txt': 1.0,\n",
              " 'MovieReviews/17902.txt': 1.0,\n",
              " 'MovieReviews/17912.txt': 2.0,\n",
              " 'MovieReviews/17933.txt': 2.0,\n",
              " 'MovieReviews/17934.txt': 1.0,\n",
              " 'MovieReviews/17945.txt': 2.0,\n",
              " 'MovieReviews/17963.txt': 2.0,\n",
              " 'MovieReviews/17971.txt': 3.0,\n",
              " 'MovieReviews/17992.txt': 1.0,\n",
              " 'MovieReviews/18004.txt': 1.0,\n",
              " 'MovieReviews/18016.txt': 1.0,\n",
              " 'MovieReviews/18032.txt': 1.0,\n",
              " 'MovieReviews/18067.txt': 1.0,\n",
              " 'MovieReviews/18068.txt': 3.0,\n",
              " 'MovieReviews/18080.txt': 3.0,\n",
              " 'MovieReviews/18087.txt': 3.0,\n",
              " 'MovieReviews/18088.txt': 2.0,\n",
              " 'MovieReviews/18136.txt': 1.0,\n",
              " 'MovieReviews/18141.txt': 1.0,\n",
              " 'MovieReviews/18156.txt': 2.0,\n",
              " 'MovieReviews/18161.txt': 1.0,\n",
              " 'MovieReviews/18181.txt': 4.0,\n",
              " 'MovieReviews/18227.txt': 1.0,\n",
              " 'MovieReviews/18263.txt': 2.0,\n",
              " 'MovieReviews/18272.txt': 4.0,\n",
              " 'MovieReviews/18273.txt': 2.0,\n",
              " 'MovieReviews/18274.txt': 1.0,\n",
              " 'MovieReviews/18282.txt': 1.0,\n",
              " 'MovieReviews/18283.txt': 1.0,\n",
              " 'MovieReviews/18307.txt': 8.0,\n",
              " 'MovieReviews/18368.txt': 3.0,\n",
              " 'MovieReviews/18376.txt': 2.0,\n",
              " 'MovieReviews/18396.txt': 1.0,\n",
              " 'MovieReviews/18406.txt': 2.0,\n",
              " 'MovieReviews/18413.txt': 2.0,\n",
              " 'MovieReviews/18414.txt': 1.0,\n",
              " 'MovieReviews/18447.txt': 3.0,\n",
              " 'MovieReviews/18473.txt': 1.0,\n",
              " 'MovieReviews/18480.txt': 1.0,\n",
              " 'MovieReviews/18485.txt': 1.0,\n",
              " 'MovieReviews/18498.txt': 3.0,\n",
              " 'MovieReviews/1858.txt': 1.0,\n",
              " 'MovieReviews/1859.txt': 1.0,\n",
              " 'MovieReviews/1860.txt': 4.0,\n",
              " 'MovieReviews/1864.txt': 1.0,\n",
              " 'MovieReviews/1865.txt': 1.0,\n",
              " 'MovieReviews/1866.txt': 1.0,\n",
              " 'MovieReviews/1867.txt': 1.0,\n",
              " 'MovieReviews/1889.txt': 1.0,\n",
              " 'MovieReviews/1891.txt': 1.0,\n",
              " 'MovieReviews/1908.txt': 1.0,\n",
              " 'MovieReviews/1910.txt': 1.0,\n",
              " 'MovieReviews/1911.txt': 1.0,\n",
              " 'MovieReviews/1912.txt': 1.0,\n",
              " 'MovieReviews/1917.txt': 3.0,\n",
              " 'MovieReviews/1921.txt': 1.0,\n",
              " 'MovieReviews/1925.txt': 1.0,\n",
              " 'MovieReviews/1928.txt': 1.0,\n",
              " 'MovieReviews/1929.txt': 1.0,\n",
              " 'MovieReviews/1930.txt': 1.0,\n",
              " 'MovieReviews/1932.txt': 1.0,\n",
              " 'MovieReviews/1944.txt': 2.0,\n",
              " 'MovieReviews/1945.txt': 2.0,\n",
              " 'MovieReviews/1961.txt': 2.0,\n",
              " 'MovieReviews/1967.txt': 1.0,\n",
              " 'MovieReviews/1968.txt': 1.0,\n",
              " 'MovieReviews/1974.txt': 1.0,\n",
              " 'MovieReviews/1975.txt': 2.0,\n",
              " 'MovieReviews/1976.txt': 3.0,\n",
              " 'MovieReviews/1981.txt': 2.0,\n",
              " 'MovieReviews/1984.txt': 1.0,\n",
              " 'MovieReviews/1985.txt': 1.0,\n",
              " 'MovieReviews/1990.txt': 3.0,\n",
              " 'MovieReviews/1994.txt': 1.0,\n",
              " 'MovieReviews/2005.txt': 1.0,\n",
              " 'MovieReviews/2006.txt': 1.0,\n",
              " 'MovieReviews/2007.txt': 1.0,\n",
              " 'MovieReviews/2008.txt': 1.0,\n",
              " 'MovieReviews/2009.txt': 1.0,\n",
              " 'MovieReviews/2025.txt': 2.0,\n",
              " 'MovieReviews/2026.txt': 1.0,\n",
              " 'MovieReviews/2030.txt': 2.0,\n",
              " 'MovieReviews/2031.txt': 1.0,\n",
              " 'MovieReviews/2036.txt': 2.0,\n",
              " 'MovieReviews/2045.txt': 1.0,\n",
              " 'MovieReviews/2046.txt': 1.0,\n",
              " 'MovieReviews/2051.txt': 1.0,\n",
              " 'MovieReviews/2055.txt': 1.0,\n",
              " 'MovieReviews/2058.txt': 1.0,\n",
              " 'MovieReviews/2059.txt': 1.0,\n",
              " 'MovieReviews/2062.txt': 2.0,\n",
              " 'MovieReviews/2076.txt': 3.0,\n",
              " 'MovieReviews/2080.txt': 1.0,\n",
              " 'MovieReviews/2081.txt': 1.0,\n",
              " 'MovieReviews/2085.txt': 1.0,\n",
              " 'MovieReviews/2086.txt': 1.0,\n",
              " 'MovieReviews/2087.txt': 2.0,\n",
              " 'MovieReviews/2089.txt': 1.0,\n",
              " 'MovieReviews/2090.txt': 1.0,\n",
              " 'MovieReviews/2091.txt': 1.0,\n",
              " 'MovieReviews/2094.txt': 1.0,\n",
              " 'MovieReviews/2095.txt': 1.0,\n",
              " 'MovieReviews/2098.txt': 1.0,\n",
              " 'MovieReviews/2099.txt': 3.0,\n",
              " 'MovieReviews/2110.txt': 1.0,\n",
              " 'MovieReviews/2113.txt': 1.0,\n",
              " 'MovieReviews/2115.txt': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dtm.get_token_occurences('director'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHrjj4L7atIV",
        "outputId": "d59b0852-9b3c-4e58-a9a5-bfbefb0c66a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to calculate entropy given a distribution\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def entropy(p):\n",
        "    if sum(p) == 0:\n",
        "        return 0\n",
        "\n",
        "    p = p/sum(p)\n",
        "\n",
        "    p = p[ p > 0 ]\n",
        "\n",
        "    H = -sum(p*np.log2(p))\n",
        "\n",
        "    return H"
      ],
      "metadata": {
        "id": "jXnIF71jav-N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "director_word_count = list(dtm.get_token_occurences('director').values())\n",
        "director_doc_count = list(dtm.get_token_occurences('director').keys())"
      ],
      "metadata": {
        "id": "WYS3fzRydMT3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#indexing files to separate authors of reviews\n",
        "berardinelli = filelists.fileids()[:80]\n",
        "schwartz = filelists.fileids()[80:180]"
      ],
      "metadata": {
        "id": "Xxemvj80emug"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the correct length\n",
        "print(len(berardinelli))\n",
        "print(len(schwartz))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx2-slfee371",
        "outputId": "dff840e6-f95d-486d-d6bb-1920b179ce42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ensuring there are no duplicates\n",
        "print(berardinelli)\n",
        "print(schwartz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR2iO166fQ6i",
        "outputId": "03c6770b-3d18-440a-868a-1119ce9d710c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MovieReviews/16748.txt', 'MovieReviews/17108.txt', 'MovieReviews/17109.txt', 'MovieReviews/17110.txt', 'MovieReviews/17111.txt', 'MovieReviews/17116.txt', 'MovieReviews/17117.txt', 'MovieReviews/17118.txt', 'MovieReviews/17119.txt', 'MovieReviews/17139.txt', 'MovieReviews/17144.txt', 'MovieReviews/17145.txt', 'MovieReviews/17146.txt', 'MovieReviews/17147.txt', 'MovieReviews/17150.txt', 'MovieReviews/17185.txt', 'MovieReviews/17192.txt', 'MovieReviews/17219.txt', 'MovieReviews/17239.txt', 'MovieReviews/17243.txt', 'MovieReviews/17254.txt', 'MovieReviews/17255.txt', 'MovieReviews/17280.txt', 'MovieReviews/17300.txt', 'MovieReviews/17303.txt', 'MovieReviews/17341.txt', 'MovieReviews/17384.txt', 'MovieReviews/17391.txt', 'MovieReviews/17398.txt', 'MovieReviews/17399.txt', 'MovieReviews/17430.txt', 'MovieReviews/17431.txt', 'MovieReviews/17447.txt', 'MovieReviews/17457.txt', 'MovieReviews/17460.txt', 'MovieReviews/17501.txt', 'MovieReviews/17518.txt', 'MovieReviews/17532.txt', 'MovieReviews/17534.txt', 'MovieReviews/17578.txt', 'MovieReviews/17609.txt', 'MovieReviews/17610.txt', 'MovieReviews/17655.txt', 'MovieReviews/17662.txt', 'MovieReviews/17663.txt', 'MovieReviews/17695.txt', 'MovieReviews/17711.txt', 'MovieReviews/17713.txt', 'MovieReviews/17753.txt', 'MovieReviews/17757.txt', 'MovieReviews/17758.txt', 'MovieReviews/17761.txt', 'MovieReviews/17803.txt', 'MovieReviews/17811.txt', 'MovieReviews/17874.txt', 'MovieReviews/17879.txt', 'MovieReviews/17886.txt', 'MovieReviews/17896.txt', 'MovieReviews/17898.txt', 'MovieReviews/17902.txt', 'MovieReviews/17912.txt', 'MovieReviews/17933.txt', 'MovieReviews/17934.txt', 'MovieReviews/17945.txt', 'MovieReviews/17963.txt', 'MovieReviews/17971.txt', 'MovieReviews/17992.txt', 'MovieReviews/18004.txt', 'MovieReviews/18016.txt', 'MovieReviews/18032.txt', 'MovieReviews/18067.txt', 'MovieReviews/18068.txt', 'MovieReviews/18080.txt', 'MovieReviews/18087.txt', 'MovieReviews/18088.txt', 'MovieReviews/18136.txt', 'MovieReviews/18141.txt', 'MovieReviews/18156.txt', 'MovieReviews/18161.txt', 'MovieReviews/18181.txt']\n",
            "['MovieReviews/18227.txt', 'MovieReviews/18263.txt', 'MovieReviews/18272.txt', 'MovieReviews/18273.txt', 'MovieReviews/18274.txt', 'MovieReviews/18282.txt', 'MovieReviews/18283.txt', 'MovieReviews/18307.txt', 'MovieReviews/18368.txt', 'MovieReviews/18375.txt', 'MovieReviews/18376.txt', 'MovieReviews/18396.txt', 'MovieReviews/18406.txt', 'MovieReviews/18413.txt', 'MovieReviews/18414.txt', 'MovieReviews/18447.txt', 'MovieReviews/18473.txt', 'MovieReviews/18480.txt', 'MovieReviews/18485.txt', 'MovieReviews/18498.txt', 'MovieReviews/1858.txt', 'MovieReviews/1859.txt', 'MovieReviews/1860.txt', 'MovieReviews/1864.txt', 'MovieReviews/1865.txt', 'MovieReviews/1866.txt', 'MovieReviews/1867.txt', 'MovieReviews/1889.txt', 'MovieReviews/1891.txt', 'MovieReviews/1908.txt', 'MovieReviews/1910.txt', 'MovieReviews/1911.txt', 'MovieReviews/1912.txt', 'MovieReviews/1916.txt', 'MovieReviews/1917.txt', 'MovieReviews/1921.txt', 'MovieReviews/1925.txt', 'MovieReviews/1928.txt', 'MovieReviews/1929.txt', 'MovieReviews/1930.txt', 'MovieReviews/1932.txt', 'MovieReviews/1934.txt', 'MovieReviews/1937.txt', 'MovieReviews/1943.txt', 'MovieReviews/1944.txt', 'MovieReviews/1945.txt', 'MovieReviews/1961.txt', 'MovieReviews/1967.txt', 'MovieReviews/1968.txt', 'MovieReviews/1974.txt', 'MovieReviews/1975.txt', 'MovieReviews/1976.txt', 'MovieReviews/1979.txt', 'MovieReviews/1981.txt', 'MovieReviews/1984.txt', 'MovieReviews/1985.txt', 'MovieReviews/1990.txt', 'MovieReviews/1991.txt', 'MovieReviews/1994.txt', 'MovieReviews/1998.txt', 'MovieReviews/2005.txt', 'MovieReviews/2006.txt', 'MovieReviews/2007.txt', 'MovieReviews/2008.txt', 'MovieReviews/2009.txt', 'MovieReviews/2025.txt', 'MovieReviews/2026.txt', 'MovieReviews/2030.txt', 'MovieReviews/2031.txt', 'MovieReviews/2033.txt', 'MovieReviews/2035.txt', 'MovieReviews/2036.txt', 'MovieReviews/2043.txt', 'MovieReviews/2045.txt', 'MovieReviews/2046.txt', 'MovieReviews/2051.txt', 'MovieReviews/2055.txt', 'MovieReviews/2058.txt', 'MovieReviews/2059.txt', 'MovieReviews/2062.txt', 'MovieReviews/2067.txt', 'MovieReviews/2076.txt', 'MovieReviews/2079.txt', 'MovieReviews/2080.txt', 'MovieReviews/2081.txt', 'MovieReviews/2085.txt', 'MovieReviews/2086.txt', 'MovieReviews/2087.txt', 'MovieReviews/2089.txt', 'MovieReviews/2090.txt', 'MovieReviews/2091.txt', 'MovieReviews/2094.txt', 'MovieReviews/2095.txt', 'MovieReviews/2098.txt', 'MovieReviews/2099.txt', 'MovieReviews/2101.txt', 'MovieReviews/2108.txt', 'MovieReviews/2110.txt', 'MovieReviews/2113.txt', 'MovieReviews/2115.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of berardinelli docs containing director\n",
        "b_count: int = sum([1 for doc in director_doc_count if doc in berardinelli])\n",
        "print('The number of berardinelli docs containing director:', b_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6g5HCfQmuJv",
        "outputId": "c595e59b-6739-45f6-9e68-78cc638d7d52"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of berardinelli docs containing director: 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of berardinelli docs NOT containing director\n",
        "b_null: int = 80 - 77\n",
        "print('The number of berardinelli docs NOT containing director:', b_null)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTO_P5xgnuri",
        "outputId": "b628d5aa-6e76-4bd7-96e6-eb3ec9e3c298"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of berardinelli docs NOT containing director: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of schwartz docs containing director\n",
        "s_count: int = sum([1 for doc in director_doc_count if doc in schwartz])\n",
        "print('The number of schwartz docs containing director:', s_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWXi-nT8njxc",
        "outputId": "e411b929-1566-4aa3-e944-05c67ab9d021"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of schwartz docs containing director: 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of schwartz docs NOT containing director\n",
        "s_null: int = 100 - 85\n",
        "print('The number of schwartz docs NOT containing director:', s_null)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDzyh2M3ntcE",
        "outputId": "22b8b785-db3c-4f5d-8398-f85144836b11"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of schwartz docs NOT containing director: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is an important step\n",
        "#make an array, rows represent \"Berardinelli\" and \"Schwartz\" respectively.\n",
        "#Columns represent the number of documents that contains the word \"director\" and the number of documents that do NOT contain the word \"director\"\n",
        "\n",
        "array = np.reshape((b_count, 80-b_count, s_count, 100-s_count),(2,2))\n",
        "array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH9tSkgWpGf3",
        "outputId": "bca6eb2d-aa30-4aef-95db-8098b6dc7cdb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[77,  3],\n",
              "       [85, 15]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the column sum of the array\n",
        "np.sum(array, axis = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf0lz8RZq5V_",
        "outputId": "54c0fd57-341b-4831-9d1d-6f8b2e17f1c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([162,  18])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1**\n",
        "Please calculate the entropy of the word “director” appearance in the 180 documents."
      ],
      "metadata": {
        "id": "mpwFh5c-rykM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the entropy of the word \"director\"\n",
        "entropy(np.array([162,18]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF5wdRSGq9i-",
        "outputId": "b8bc72ac-84db-4437-e444-4d9f73906435"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4689955935892812"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the row sum of the array\n",
        "np.sum(array, axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYIxSX5LseKD",
        "outputId": "31d17b14-a7b5-4099-c8be-23392c9c3c44"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 80, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the entropy of the author (berardinelli, schwartz)\n",
        "entropy_class = entropy(np.sum(array, axis = 1))\n",
        "entropy_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSpIRV6NsgRG",
        "outputId": "6b8e1ed3-f99f-4d48-d880-9f45d40c28cf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9910760598382222"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the column probabilities\n",
        "column_probs = np.sum(array, axis = 0)/180\n",
        "column_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16TpsMK9sqQx",
        "outputId": "1dc0c666-0012-486f-8ea8-f7138f0c1f1b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9, 0.1])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the column entropies\n",
        "column_entropy = np.apply_along_axis(entropy, 0, array)\n",
        "column_entropy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OQAfhtys5C_",
        "outputId": "d7a41f6f-ceaa-4b63-cc89-bb72cb54b539"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.99824017, 0.65002242])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate conditional entropy\n",
        "conditional_entropy = sum(column_probs*column_entropy)\n",
        "conditional_entropy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdW8Zve0tIgF",
        "outputId": "923e42a5-df03-4381-aeee-e01a8555acdb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9634183936209307"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2**\n",
        "Calculate the mutual information between the “director” and the document author.\n"
      ],
      "metadata": {
        "id": "9pcED0yEtwlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the mutual information between the word \"director\" and the author\n",
        "mutual_information = entropy_class - conditional_entropy\n",
        "mutual_information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRzG2dFotQNz",
        "outputId": "52d56045-e658-411f-e10c-a4d9d09a3c1d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.027657666217291488"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3**\n",
        "Find the top ten words with the highest mutual information with the document author and their respective mutual information. Explain (in Python comments) that what it means for a word by having a high mutual information with the document author.\n"
      ],
      "metadata": {
        "id": "5KrQehD1vcxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = {}\n",
        "for document in corpus:\n",
        "    for word in document:\n",
        "        if word not in vocabulary:\n",
        "            vocabulary[word] = 1\n",
        "        else:\n",
        "            vocabulary[word] += 1"
      ],
      "metadata": {
        "id": "pFY3YlQpYyZf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_association(word):\n",
        "  director_word_count = list(dtm.get_token_occurences(word).values())\n",
        "  director_doc_count = list(dtm.get_token_occurences(word).keys())\n",
        "\n",
        "  print('Entropy of `{}` in the documents:'.format(word), entropy(director_word_count))\n",
        "\n",
        "  b_count: int = sum([1 for doc in director_doc_count if doc in berardinelli])\n",
        "  print('The number of berardinelli docs containing {}:'.format(word), b_count)\n",
        "\n",
        "  s_count: int = sum([1 for doc in director_doc_count if doc in schwartz])\n",
        "  print('The number of schwartz docs containing {}:'.format(word), s_count)\n",
        "\n",
        "  array = np.reshape((b_count, 80-b_count, s_count, 100-s_count),(2,2))\n",
        "  print('Matrix:', array)\n",
        "\n",
        "  entropy_class = entropy(np.sum(array, axis = 1))\n",
        "  print('Entropy Class:', entropy_class)\n",
        "\n",
        "  column_probs = np.sum(array, axis = 0)/180\n",
        "  print('Column Probabilities:', column_probs)\n",
        "\n",
        "  column_entropy = np.apply_along_axis(entropy, 0, array)\n",
        "  print('Column Entropy:', column_entropy)\n",
        "\n",
        "  conditional_entropy = sum(column_probs*column_entropy)\n",
        "  print('Conditional Entropy:', conditional_entropy)\n",
        "\n",
        "  mutual_information = entropy_class - conditional_entropy\n",
        "  print('Mutual Information:', mutual_information)\n"
      ],
      "metadata": {
        "id": "Dm9gO_V4kByP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "there = word_association('there')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny9HdvMSkY8B",
        "outputId": "44484f83-df31-461f-d28e-8f23277f7cbb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of `there` in the documents: 5.322579324069085\n",
            "The number of berardinelli docs containing there: 0\n",
            "The number of schwartz docs containing there: 50\n",
            "Matrix: [[ 0 80]\n",
            " [50 50]]\n",
            "Entropy Class: 0.9910760598382222\n",
            "Column Probabilities: [0.27777778 0.72222222]\n",
            "Column Entropy: [-0.         0.9612366]\n",
            "Conditional Entropy: 0.6942264367442992\n",
            "Mutual Information: 0.29684962309392304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "right = word_association('right')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG5-s_FbkfV9",
        "outputId": "393682b5-bc76-4e93-fab7-09df15bfeb16"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of `right` in the documents: 6.679392948034621\n",
            "The number of berardinelli docs containing right: 79\n",
            "The number of schwartz docs containing right: 40\n",
            "Matrix: [[79  1]\n",
            " [40 60]]\n",
            "Entropy Class: 0.9910760598382222\n",
            "Column Probabilities: [0.66111111 0.33888889]\n",
            "Column Entropy: [0.92107152 0.12068101]\n",
            "Conditional Entropy: 0.6498280710388034\n",
            "Mutual Information: 0.34124798879941887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_association_final(word):\n",
        "  director_word_count = list(dtm.get_token_occurences(word).values())\n",
        "  director_doc_count = list(dtm.get_token_occurences(word).keys())\n",
        "\n",
        "  entropy_value = entropy(director_word_count)\n",
        "\n",
        "  b_count: int = sum([1 for doc in director_doc_count if doc in berardinelli])\n",
        "\n",
        "  s_count: int = sum([1 for doc in director_doc_count if doc in schwartz])\n",
        "\n",
        "  array = np.reshape((b_count, 80-b_count, s_count, 100-s_count),(2,2))\n",
        "\n",
        "  entropy_class = entropy(np.sum(array, axis = 1))\n",
        "\n",
        "  column_probs = np.sum(array, axis = 0)/180\n",
        "\n",
        "  column_entropy = np.apply_along_axis(entropy, 0, array)\n",
        "\n",
        "  conditional_entropy = sum(column_probs*column_entropy)\n",
        "\n",
        "  mutual_information = entropy_class - conditional_entropy\n",
        "  return mutual_information\n",
        "\n",
        "mi_scores = {}\n",
        "for word in vocabulary.keys():\n",
        "    mi_score = word_association_final(word)\n",
        "    mi_scores[word] = mi_score\n",
        "\n",
        "sorted_scores = sorted(mi_scores.items(), key = lambda x: x[1], reverse = True)\n",
        "\n",
        "top_ten = [word for word, _ in sorted_scores[:10]]\n",
        "\n",
        "print('Words with Highest MI:')\n",
        "for word, mi_score in sorted_scores [:10]:\n",
        "  print(f'{word}:{mi_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWak0PvdYfvl",
        "outputId": "f41e2b37-d389-4ee5-b5e7-519caffd816d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words with Highest MI:\n",
            "reserv:0.5360547127375266\n",
            "denni:0.5177766824991015\n",
            "schwartz\n",
            ":0.49202296056300543\n",
            "right:0.34124798879941887\n",
            "there:0.29684962309392304\n",
            "howev:0.27421790253727896\n",
            "releas:0.2702108404097968\n",
            "cast:0.2564686677667559\n",
            "screenplai:0.2564686677667559\n",
            "produc:0.23020448708340835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain what it means for a word by having a high mutual information with the document author.**"
      ],
      "metadata": {
        "id": "lJCajfvWnY-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutual Information provides a way to discover syntagmatic relationships. If an author has high mutual information between some words, it will be easier to identify what syntax or words they use in their work as well as better identify their writing style or tendencies. We can know what topics they prefer, and what they like to evaluate."
      ],
      "metadata": {
        "id": "yKgaXOxyndc-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erN_7BR4oefP"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}